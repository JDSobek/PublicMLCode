{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"UsingPyTorch.ipynb","provenance":[],"collapsed_sections":["sitting-projector","differential-lounge","spoken-pound","t_z_ufCv1UOD","k0hA4diA1bXR","empty-hacker","lwDmiNLsT989","huuDGWEIp-cn","C-jyyxmytPfu","christian-flour","100dGxmFbmrx","pretty-jacob","yZ0vLTftDNih","4MgL2580DHIB","DxOwpSWsD7Lx","NcXHqeQ_D_DT","EHrm6zUHEDrQ"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sitting-projector"},"source":["# PyTorch Introduction"],"id":"sitting-projector"},{"cell_type":"markdown","metadata":{"id":"sealed-expansion"},"source":["This is intended to be a basic introduction to using pure PyTorch for machine learning.  Specifically, the focus is building and training deep learning models using PyTorch.  Make sure you choose a GPU accelerated Colab runtime before running any code cells.\n","\n","Working with PyTorch comes with certain disadvantages compared to other deep learning frameworks such as MONAI, PyTorch Lightning, TensorFlow, or fast.ai.  Frameworks like these tend to abstract away much of the underlying code required to set up deep learning models, allowing you to quickly create and train new models by calling just a few functions.  PyTorch handles the creation of most of the basic elements needed to build and train models, but requires the programmer to assemble these elements into a training process.  Because you are generally required to write more code and have a better understanding of deep learning concepts, there's much more room for you to make mistakes and a higher barrier to entry for new researchers\n","\n","So why work with PyTorch?  PyTorch's lower level approach provides direct control over how your model is fed data and what happens inside your model. Once you've understood how to set up the model training process in PyTorch, it's straightforward to make changes and experiment.  It can be quite complicated to expand the functionality of other, higher level frameworks or take full advantage of their features.  PyTorch, however, makes this relatively straightforward and as a result is an extremely flexible framework, so flexible that every aforementioned framework except Tensorflow is built on top of PyTorch.\n","\n","As an example problem you might face, most deep learning frameworks natively handle natural images (RGB images saved in formats like png or jpg), but do not come with methods to handle medical imaging formats such as NIfTI or DICOM.  PyTorch does not natively handle these formats either, but because you can construct a custom `Dataset` and `DataLoader` for your model to use, a few extra lines of code can convert your imaging data into something your PyTorch model can interpret.  In comparison, other frameworks sometimes require extensive work or lengthy data conversion processes to handle unexpected data formats.\n","\n","Finally, for the experienced Python programmer, PyTorch is widely regarded as very Pythonic.  Skills, conventions, and good habits learned in programming Python code transfer directly to coding with PyTorch.  PyTorch places few constraints on how the functions and classes you write must be constructed and allows you to easily integrate other libraries into your code.\n","\n","[Official PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n","\n","[Official PyTorch Tutorials](https://pytorch.org/tutorials/index.html)\n","\n","For a deeper dive into PyTorch, [this book](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf) is recommended."],"id":"sealed-expansion"},{"cell_type":"code","metadata":{"id":"7TQo0eNKJVU_"},"source":["# for the first part of this chapter we'll only need to import PyTorch and Torchvision\n","import torch\n","import torchvision"],"id":"7TQo0eNKJVU_","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"differential-lounge"},"source":["# Torchvision"],"id":"differential-lounge"},{"cell_type":"markdown","metadata":{"id":"stunning-nashville"},"source":["Before we get into the details of model construction, it's worth briefly mentioning the torchvision package that you should have installed alongside PyTorch if you followed [PyTorch's installation instructions](https://pytorch.org/get-started/locally/).\n","\n","To aid with computer vision tasks, many deep learning frameworks come with pre-built and pre-trained neural networks, pre-defined data transformation and augmentation methods, and routines to download commonly used datasets.  Torchvision is the PyTorch equivalent to this.  As seen below, this provides a very simple method for instantiating a model with PyTorch."],"id":"stunning-nashville"},{"cell_type":"code","metadata":{"id":"canadian-salmon"},"source":["model = torchvision.models.resnet18(pretrained=True)"],"id":"canadian-salmon","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"essential-parks"},"source":["These models come with a `pretrained` parameter that allows you to determine whether or not your model's weights and biases will be randomly initialized or initialized with weights and biases generated from training on an existing dataset, generally one derived from ImageNet.\n","\n","While easy to create, models created using these preexisting architectures strongly constrain your data at both the input and output sides of your network.  They also influence how easy it is to perform certain advanced, or niche, tasks (such as reading intermediate activations).  This varies on a case-by-case basis for each model included in the package.\n","\n","It's possible to work around these restrictions, or avoid them altogether.  To work towards that, we will begin looking at how custom model creation is done using PyTorch and come back to torchvision later in the chapter."],"id":"essential-parks"},{"cell_type":"markdown","metadata":{"id":"spoken-pound"},"source":["# Tensors"],"id":"spoken-pound"},{"cell_type":"markdown","metadata":{"id":"rural-constitutional"},"source":["Mathematically, tensors are generalized version of matrices and vectors.  Put another way, vectors are 1 dimensional tensors and matrices are 2 dimensional tensors.  Tensors of 3 dimensions or more are just called tensors.\n","\n","`torch.Tensor`s are largely true to this definition.  They are very similar to NumPy arrays but come with significantly expanded functionality.\n","\n","Most processes in PyTorch are built on and run on tensors.  Your models' parameters are tensors, the data the model acts on are tensors, and the model's raw output will be tensors.  PyTorch contains several utilities to help you convert your data into tensors.  `torch.from_numpy`, which converts NumPy arrays to torch tensors, is one of the most commonly used.\n","\n","Let's look at several ways to create tensors in PyTorch."],"id":"rural-constitutional"},{"cell_type":"code","metadata":{"id":"congressional-saskatchewan"},"source":["one_hundred = [100]\n","one_hundred_tensor = torch.Tensor(one_hundred) # creating a tensor using preexisting data\n","ones_tensor = torch.ones((2,2)) # creating a 2x2 tensor with every value initialized to 1\n","zeros_tensor = torch.zeros((2,3,4)) # creating a 2x3x4 tensor with every value initialized to 0\n","print(one_hundred_tensor)\n","print(ones_tensor)\n","print(zeros_tensor)"],"id":"congressional-saskatchewan","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eleven-damage"},"source":["For the most part, operations on `torch.Tensor`s behave like operations on NumPy arrays.  You can perform most basic mathematical operations on them, such as addition/subtraction/multiplication of scalars or other tensors.  `torch.Tensor`s also have properties familiar from NumPy arrays, such as shape and datatype.  However there are a couple new, especially important, properties that should be discussed, which are their gradient and device.\n","\n"],"id":"eleven-damage"},{"cell_type":"markdown","metadata":{"id":"t_z_ufCv1UOD"},"source":["## Device"],"id":"t_z_ufCv1UOD"},{"cell_type":"markdown","metadata":{"id":"_ETUoVhR1Uu5"},"source":["Device is fairly simple, it's a property that tells your program whether a tensor exists in GPU memory (these devices are generally named cuda:#) or in system RAM (devices named cpu).  NumPy arrays, for example, are always stored in system memory and always run calculations on your CPU, and a different library, such as CuPy, is required to do NumPy-like calculations on a GPU.  PyTorch allows us to create tensors and run calculations on them using either type of device, and allows us transfer them between devices with a simple function call.\n","\n","Note that an error will occur if you attempt to create a cuda `Tensor` while your runtime environment does not have access to a GPU."],"id":"_ETUoVhR1Uu5"},{"cell_type":"code","metadata":{"id":"confirmed-profit"},"source":["a_tensor = torch.zeros((2,2)) # creating a tensor on the CPU\n","b_tensor = torch.zeros((2,2), device='cuda:0') # creating a tensor on GPU 0\n","print('a_tensor device: ' + str(a_tensor.device))\n","print('b_tensor device: ' + str(b_tensor.device))"],"id":"confirmed-profit","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hispanic-arctic"},"source":["a_tensor = a_tensor.to('cuda:0') # moving the CPU tensor onto GPU 0\n","b_tensor = b_tensor.to('cpu') # moving the GPU tensor onto the CPU\n","print('a_tensor device: ' + str(a_tensor.device))\n","print('b_tensor device: ' + str(b_tensor.device))"],"id":"hispanic-arctic","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k0hA4diA1bXR"},"source":["## Gradient"],"id":"k0hA4diA1bXR"},{"cell_type":"markdown","metadata":{"id":"regional-artwork"},"source":["Unlike NumPy arrays and similar data structures, PyTorch will automatically track gradients for `torch.Tensor`s.  An in-depth discussion on what this means is available [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html).\n","\n","What this behind the scenes gradient tracking does is abstract away several calculations and make it simple to update your model's parameters during the training process.  It's possible to turn it off for a `Tensor` by setting the `requires_grad` parameter to `False`, or turn it off for calculations by running them within a `with torch.no_grad():` statement.  This is useful for data preprocessing and augmentation on input data or reducing memory requirements and speeding up operations that don't make use of the gradient.\n","\n","It's probably easiest to understand how to use the gradient by seeing it in a live example, so keep an eye on the comments in the code below as we build a training loop."],"id":"regional-artwork"},{"cell_type":"markdown","metadata":{"id":"empty-hacker"},"source":["# Creating Layers"],"id":"empty-hacker"},{"cell_type":"markdown","metadata":{"id":"c3EkqIcU1IOx"},"source":["Creating neural network layers in PyTorch is fairly straightforward."],"id":"c3EkqIcU1IOx"},{"cell_type":"code","metadata":{"id":"immune-backing"},"source":["conv1 = torch.nn.Conv2d(3, 64, (3,3))"],"id":"immune-backing","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"McP6-NxP1j-6"},"source":["`conv1` is the variable name assigned to this layer, which is of type `torch.nn.Conv2d`.  [Looking at the documentation for this type of layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), we see that the first input parameter, 3, is the number of channels the layer needs as input, the second input parameter, 64, is the number of channels the layer will output, and the spatial size of the kernel this convolutional layer uses is (3,3).\n","\n","PyTorch includes [a great variety of layers](https://pytorch.org/docs/stable/nn.html).  For computer vision purposes the most important layers are Convolution layers and Activation layers, but many other types of layers are used to improve training results."],"id":"McP6-NxP1j-6"},{"cell_type":"markdown","metadata":{"id":"owned-heritage"},"source":["# Building and Training a Simple Model from Scratch"],"id":"owned-heritage"},{"cell_type":"markdown","metadata":{"id":"kj5t3HUR-ZpK"},"source":["In this section we will construct and train a simple model so we can see the full training process in PyTorch.  The PyTorch beginner tutorials contain a similar [example classifier for CIFAR-10](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html), on which much of the following code is based.  The code in our version is intended to be written in a more compartmentalized and more verbose way that should be easier to understand and extend more easily to complex projects."],"id":"kj5t3HUR-ZpK"},{"cell_type":"markdown","metadata":{"id":"lwDmiNLsT989"},"source":["## Downloading CIFAR-10 and building a DataLoader from a Dataset"],"id":"lwDmiNLsT989"},{"cell_type":"markdown","metadata":{"id":"B0tn5WyrT99C"},"source":["Before building a model we need data and a problem to analyze.  For this example we are using the CIFAR-10 dataset.  This dataset consists of thousands of 32x32 color images of 10 different classes of object, as described on the [CIFAR website](https://www.cs.toronto.edu/~kriz/cifar.html).  The problem our model must resolve is identifying which class each image in our dataset belongs to.  Reasonable accuracy on this dataset can be achieved even with simple convolutional neural networks, so it's ideal for sketching the basic PyTorch pipeline for model training.\n","\n","In order to pass data into our model, we must create a `Dataset` and a `DataLoader`.  You can think of these as two tools that work together to load files and feed them into your model.  The `Dataset` determines how data is acquired, which usually means how files are read and how the data within those files is preprocessed.  Most operations that you wish to perform on individual data objects, for instance some forms of normalization, are best done in the `Dataset`.  The `DataLoader` handles assembling batches of input from examples returned by the `Dataset`.\n","\n","Fortunately, PyTorch comes with a pre-built CIFAR-10 Dataset that will download the data for us.  We'll cover how to create a custom `torch.utils.data.Dataset` later.  `torch.utils.data.DataLoader`s can be created directly from a `Dataset`, and there's generally no need to write a custom method."],"id":"B0tn5WyrT99C"},{"cell_type":"code","metadata":{"id":"TuTZQfTLT99G"},"source":["# torchvision.transforms.ToTensor() tells PyTorch to convert the images in the CIFAR-10 dataset into torch.Tensors\n","transform = torchvision.transforms.ToTensor()\n","\n","# download (if needed) and create a CIFAR-10 dataset for training\n","train_dataset = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n","# use the training dataset to create a training DataLoader\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n","\n","# download (if needed) and create a CIFAR-10 dataset for validation\n","val_dataset = torchvision.datasets.CIFAR10('./data', train=False, download=True, transform=transform)\n","# use the validation dataset to create a validation DataLoader\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=True)"],"id":"TuTZQfTLT99G","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"boJwAqhjncdP"},"source":["As a general guideline, operations that happen within a `Dataset` should be limited to operations on CPU `Tensor`s, and operations that you'd prefer to perform on on CUDA `Tensor`s, such as most augmentation methods, should happen after the `DataLoader` assembles a batch and transfers the data to the GPU.  This helps speed up the training process by facilitating PyTorch multiprocessing routines and reducing the number of times data is transferred between your CPU and GPU, which is generally slow.  This is a peek ahead at advanced features, so make a mental note for the future, when you're creating your own datasets."],"id":"boJwAqhjncdP"},{"cell_type":"markdown","metadata":{"id":"huuDGWEIp-cn"},"source":["## Defining the Model Architecture"],"id":"huuDGWEIp-cn"},{"cell_type":"markdown","metadata":{"id":"DYl8Deyp9SjH"},"source":["`torch.nn.Module` is the Python `class` that most neural networks and layers in PyTorch are based on.  If we define our neural network as a subclass of `torch.nn.Module`, PyTorch will automatically take care of most of the background methods required for our model to run.  While a general discussion on class inheritance in Python is out of place for this chapter, know that because of it all we need to do is define an `__init__` and `forward` method to build our model."],"id":"DYl8Deyp9SjH"},{"cell_type":"code","metadata":{"id":"OHqr94KCrkMX"},"source":["# defining our CIFAR-10 model\n","class CIFAR10_Net(torch.nn.Module):\n","  # __init__ tells Python what to do when an object of the CIFAR10_Net class is created\n","  # in this case, it creates the layers that make up our network\n","  def __init__(self):\n","    # this defines our model as a subclass of torch.nn.Module\n","    # as a result, important functions such as backward() that we will see later are defined for us\n","    super(CIFAR10_Net, self).__init__()\n","    # because CIFAR-10 contains RGB images, our input layer must accept 3-channel images\n","    self.in_layer =  torch.nn.Conv2d(3, 8, (5,5))\n","    # we'll create one hidden convolutional layer\n","    self.hidden_conv =  torch.nn.Conv2d(8, 16, (5,5))\n","    # we add MaxPool2d layers to reduce the number of computations we need to make\n","    # and reduce the model's memory footprint\n","    self.pool = torch.nn.MaxPool2d((2, 2), 2)\n","    # a Flatten layer is the simplest way to convert a 2 or more dimensional array\n","    # into a 1 dimensional array\n","    self.flat = torch.nn.Flatten()\n","    # we'll also have one hidden fully connected layer to help classify the images\n","    # the number of input nodes required are 16 (channels) * 5 (kernel height) * 5 (kernel width) = 400\n","    self.hidden_fc = torch.nn.Linear(400, 100)\n","    # creating the output layer\n","    # because there are 10 classes in CIFAR-10, the output layer needs 10 output nodes\n","    self.out_layer = torch.nn.Linear(100, 10)\n","    # we'll use ReLU as our activation function throughout the model\n","    self.relu = torch.nn.ReLU()\n","  \n","  # the forward method defines the forward pass of data through our model\n","  # the backward method, created for us by PyTorch, wil define the backward\n","  # pass based on the forward method\n","  def forward(self, x):\n","    # input layer, activation, and pooling\n","    x = self.in_layer(x)\n","    x = self.relu(x)\n","    x = self.pool(x)\n","    # hidden convolution layer, activation, and pooling\n","    x = self.hidden_conv(x)\n","    x = self.relu(x)\n","    x = self.pool(x)\n","    # flattening the output of the convolutional part of the model\n","    # so it can be fed into the fully connected layers\n","    x = self.flat(x)\n","    # hidden fully connected layer and activation\n","    x = self.hidden_fc(x)\n","    x = self.relu(x)\n","    # output layer\n","    # note the model does not contain a softmax layer for final activation\n","    # the advantage to placing the final activation outside the model\n","    # is that certain loss functions incorporate final activations in their calculation\n","    # and models that don't contain final activations\n","    # can more easily test different loss functions\n","    x = self.out_layer(x)\n","    return x\n","\n","# tell Python to create an instance of our CIFAR10_Net class on the GPU\n","net = CIFAR10_Net().cuda()\n","\n","# if you do not want to run your model on GPU, define your model this way instead\n","# PyTorch assumes tensors should be created on the CPU unless you tell it otherwise\n","#net = CIFAR10_Net()"],"id":"OHqr94KCrkMX","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C-jyyxmytPfu"},"source":["## Defining the Loss Function and Optimizer"],"id":"C-jyyxmytPfu"},{"cell_type":"markdown","metadata":{"id":"cvA26xRuNOba"},"source":["PyTorch comes with many predefined loss functions (often referred to as `criterion`) and optimizers for your gradient updates.  Custom loss functions (such as those based on the Dice coefficient for segmentation) are commonly needed for medical imaging problems, but for the current problem built-in PyTorch methods are sufficient."],"id":"cvA26xRuNOba"},{"cell_type":"code","metadata":{"id":"0j4FKYTVtTI4"},"source":["# we use crossentropy loss for multi-category classification problems\n","# note that the PyTorch implementation of crossentropy loss calculates the log softmax before calculating the crossentropy\n","criterion = torch.nn.CrossEntropyLoss()\n","# Adam is one of the more commonly used optimizers, so we'll use that for this problem\n","optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)"],"id":"0j4FKYTVtTI4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"christian-flour"},"source":["## Training a Simple Model"],"id":"christian-flour"},{"cell_type":"markdown","metadata":{"id":"J95JXq3i6mQQ"},"source":["Here we make the largest divergence from the official PyTorch CIFAR-10 tutorial.  Rather than writing our training and validation passes as part of the `__main__` script, we will define two functions, one for training and one for validation, that control the behavior during each epoch.\n","\n","While this change adds some complication, the additional compartmentalization will make our code more readable and easier to add functionality to, especially as the code grows more complicated.  It also helps prevent memory leaks that can happen when your code uses both multiprocessing and cuda."],"id":"J95JXq3i6mQQ"},{"cell_type":"code","metadata":{"id":"xV75PBCyRFGm"},"source":["# Defining what happens during one epoch's worth of training\n","# Passing in our DataLoader, model, loss function and optimizer like this\n","# is not problem for PyTorch\n","def train_pass(data_loader, model, loss_fn, optim):\n","  # first we make sure the model is in train mode\n","  # so it will track gradients and unfreeze parameters\n","  model.train()\n","  # we define a running loss variable so we can track our model's progress\n","  running_loss =  0.0\n","  # now we define what happens as the model goes through the data within the DataLoader\n","  # this will iterate over the batches defined within the DataLoader\n","  for i, data in enumerate(data_loader, 0):\n","      # read the inputs, a list of [inputs, labels], from the batch\n","      inputs, labels = data\n","      # because we want to train on the GPU\n","      # we must send our inputs and labels to the GPU\n","      # comment out both of these lines if you prefer to run on the CPU\n","      inputs = inputs.cuda()\n","      labels = labels.cuda()\n","\n","      # reset the gradients for every parameter in our optimizer to zero\n","      # so that we don't mix the loss for the current batch with the last batch\n","      optim.zero_grad()\n","\n","      # get the outputs by sending our inputs through one forward pass of the model\n","      outputs = model(inputs)\n","      # calculate the loss by comparing our model's outputs to their labels\n","      # note: if our loss function did not include a softmax operation (or other final activation)\n","      # we would need to make sure to calculate it before calculating the loss\n","      loss = loss_fn(outputs, labels)\n","      # backward() calculates the gradient from the model's loss and stores it in\n","      # the model's tensors' gradient attribute\n","      loss.backward()\n","      # step() tells the optimizer to update our model's parameters\n","      optim.step()\n","\n","      # add the loss for the current batch to our running loss variable\n","      running_loss += loss.item()\n","\n","      # this controls how we want the training loop to report our model's progress\n","      # in this case every 100 batches we want it to print, then reset, the average loss\n","      if i % 100 == 99:\n","        print('Training loss: ' + str(running_loss/100))\n","        running_loss = 0.0\n","\n","  # because all of the parameters that are updated are defined\n","  # outside the training loop we don't need this function to return anything\n","  # this line doesn't actually do anything, but is included just to make it clear\n","  # where this function ends\n","  return None"],"id":"xV75PBCyRFGm","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AWQXDueMRWGV"},"source":["# Now we'll define what happens when an epoch reaches the validation stage\n","# Note that we don't need to pass in the optimizer because we don't intend\n","# to update our model's parameters\n","def valid_pass(data_loader, model, loss_fn):\n","  # first we set the model to eval mode\n","  # this disables several under the hood operations that we won't need like gradient tracking\n","  # and will speed up the calculations within\n","  model.eval()\n","  # like before we define a loss variable to keep track of how our model performs\n","  val_loss = 0.0\n","\n","  # because we don't want to calculate or track any gradients during validation\n","  # we tell PyTorch not to track any gradients for these calculations using\n","  # with torch.no_grad():\n","  with torch.no_grad():\n","    # now we define what happens as our model runs through the validation DataLoader\n","    for i, data in enumerate(data_loader, 0):\n","      # read the inputs, a list of [inputs, labels], from the batch\n","      inputs, labels = data\n","      # because we want to train on the GPU\n","      # we must send our inputs and labels to the GPU\n","      # comment out both of these lines if you prefer to run on the CPU\n","      inputs = inputs.cuda()\n","      labels = labels.cuda()\n","\n","      # Note the lack of a backwards pass in the validation loop\n","      # because we're not going to perform any parameter updates or gradient calculations\n","\n","      # get the outputs by sending our inputs through one forward pass of the model\n","      outputs = model(inputs)\n","      # calculate the loss by comparing our model's outputs to their labels\n","      # note: if our loss function did not include a softmax operation (or other final activation)\n","      # we would need to make sure to calculate it before calculating the loss\n","      loss = loss_fn(outputs, labels)\n","\n","      # add the loss for the current batch to our validation loss\n","      val_loss += loss.item()\n","\n","  # Finally we'll have our validation loop print the average loss of a validation batch\n","  print('Validation loss: ' + str(val_loss / len(data_loader)))\n","\n","  # this again does nothing but mark where the function ends\n","  return None"],"id":"AWQXDueMRWGV","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"revised-berkeley"},"source":["# Now we will actually run through model training and train for 5 epochs\n","for epoch in range(5):\n","  print('Epoch Number: ' + str(epoch))\n","  print('Entering training loop:')\n","  train_pass(train_loader, net, criterion, optimizer)\n","  print('Entering validation loop:')\n","  valid_pass(val_loader, net, criterion)\n","print('Finished Training')"],"id":"revised-berkeley","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"100dGxmFbmrx"},"source":["## Saving the results of our training"],"id":"100dGxmFbmrx"},{"cell_type":"markdown","metadata":{"id":"Fhr4keQEd6Wu"},"source":["The training process for this model was quite fast, but that won't always be the case.  Fortunately it's pretty easy to save PyTorch models for future use."],"id":"Fhr4keQEd6Wu"},{"cell_type":"code","metadata":{"id":"qRaNNadGbkvV"},"source":["# tell PyTorch where we want to save our model and what to call it\n","model_path = './CIFAR_Net.pth'\n","# we save our model's state_dict(), which save the model's parameters and tells PyTorch which layer they belong to\n","# but does not save our model's architecture, we need to make sure that is defined before we load a state_dict into it\n","torch.save(net.state_dict(), model_path)"],"id":"qRaNNadGbkvV","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pretty-jacob"},"source":["## Running Inference Using a Simple Model"],"id":"pretty-jacob"},{"cell_type":"markdown","metadata":{"id":"uGa3Rq2NdxD6"},"source":["Before we run inference, let's create a new model and load the `state_dict` we saved earlier, so that we'll know how to do it in the future."],"id":"uGa3Rq2NdxD6"},{"cell_type":"code","metadata":{"id":"featured-bryan"},"source":["# tell PyTorch where the model is saved\n","stored_model_path = './CIFAR_Net.pth'\n","# define the architecture of the network we will use for our inference\n","# we would need to include the code that defines CIFAR10_Net()\n","# if this were a new notebook or script making use of a model we had trained earlier\n","test_net = CIFAR10_Net()\n","# finally we tell PyTorch to load the state_dict from the file we saved earlier\n","# and to load it into the state_dict of our inference model\n","test_net.load_state_dict(torch.load(stored_model_path))"],"id":"featured-bryan","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XfdOhhCXKfeY"},"source":["Because this is just an example problem and we want to keep it simple, we'll reuse the validation `DataLoader` for our inference task even though this is a bad idea when solving real problems."],"id":"XfdOhhCXKfeY"},{"cell_type":"code","metadata":{"id":"PJ5YGEsLds_z"},"source":["test_loader = val_loader"],"id":"PJ5YGEsLds_z","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TeL3zRnmPPUA"},"source":["And now we can calculate our model's accuracy on the test dataset."],"id":"TeL3zRnmPPUA"},{"cell_type":"code","metadata":{"id":"e2l1h_POLMV4"},"source":["# creating a variable to track the number of images our model classified correctly\n","correct = 0\n","# getting the total number of images our model will be classifying\n","total = len(test_loader.dataset)\n","# as with the validation set we don't want PyTorch to calculate gradients\n","with torch.no_grad():\n","  # iterating over the test  set DataLoader\n","  for data in test_loader:\n","    # grab the images and labels from the batch\n","    images, labels = data\n","    # record the model's outputs\n","    # note that this time we run the model's outputs through a softmax function first\n","    # since we won't be using a loss function that does so automatically\n","    outputs = torch.nn.functional.softmax(test_net(images), dim=1)\n","    # finally we used an argmax function to convert our outputs into predicted classes\n","    predicted = torch.argmax(outputs, dim=1)\n","    # add the number of correct guesses we made for this batch to the total correct\n","    correct += (predicted == labels).sum().item()\n","\n","# Finally, report our model's accuracy\n","accuracy = correct/total\n","print('Accuracy of the network on the test images: ' + str(accuracy))"],"id":"e2l1h_POLMV4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mPvc5GPqZsHZ"},"source":["46% is not an impressive accuracy, but considering we gave a very small model only a handful of epochs to train for, the model is doing much better than the 10% accuracy random guessing would give us.\n","\n","There are two main things you should notice in this inference loop.  First is that the process used for running inference on your model is nearly identical to the validation loop we used during training.  You could even skip creating a test `DataLoader` and `Dataset`, as long as you make sure the data you pass into your model has had the same pre-processing performed on it that the batches from your `DataLoader` had while you were training.\n","\n","Second is that we only used the CPU for the inference calculations.  Because we're not going to be calculating gradients or updating our model's parameters, there's relatively little to gain from pushing our computations to the GPU.  Since the model we're using is so small, the extra overhead needed to transfer data to the GPU memory may actually slow down our inference process."],"id":"mPvc5GPqZsHZ"},{"cell_type":"markdown","metadata":{"id":"KWj0Voxx8Gez"},"source":["# Segmentation with Advanced PyTorch"],"id":"KWj0Voxx8Gez"},{"cell_type":"markdown","metadata":{"id":"UThzcJlu9bJg"},"source":["Now that we've built a relatively simple model in PyTorch to solve a relatively simple problem, we can move on to a harder, medically-relevant problem.  In this example we will train a segmentation model that looks at images of the abdomen and segment the pancreas.  In it we'll cover many issues we were able to avoid in the CIFAR-10 example, such as defining custom loss functions, defining a custom `Dataset`, constructing models with more complicated architectures, and some tricks to speed up training times.\n","\n","Note: If you don't fully understand the previous example review and explore the concepts discussed during it.  This example expands on the code from that example and is substantially more complex.\n","\n","The abdominal CT images and segmentation masks used in this example are provided courtesy of [The Cancer Imaging Archive](https://www.cancerimagingarchive.net/)."],"id":"UThzcJlu9bJg"},{"cell_type":"markdown","metadata":{"id":"yZ0vLTftDNih"},"source":["## Downloading and Organizing our Data"],"id":"yZ0vLTftDNih"},{"cell_type":"markdown","metadata":{"id":"rcqAtMIXDnWS"},"source":["For this example the data is stored on an existing GitHub repository.  The following code downloads the images and segmentation masks and places them into labeled folders.\n","\n","While building custom `Dataset`s gives you a lot of freedom in how you organize your data, the method used to organize the data here provides very few programming challenges.  Images and labels are stored separately, with labels named identically to the images except for a tag (in this case \"-Mask\") that can be easily picked out in your `Dataset`, and each different set of data, for training, validation, and testing, is stored in separate directories so your `Dataset` can simply consider every file of the appropriate type it sees as valid."],"id":"rcqAtMIXDnWS"},{"cell_type":"code","metadata":{"id":"rFrVQ11_8Mcr"},"source":["# Now we'll download and organize our data\n","# First things first, we clean up any residual folders and data\n","# that may have existed from an earlier attempt to download the data\n","!rm -rf ./MC4-TensorflowUNet\n","\n","!rm -rf trainimages\n","!mkdir trainimages\n","!rm -rf trainmasks\n","!mkdir trainmasks\n","\n","!rm -rf validationimages\n","!mkdir validationimages\n","!rm -rf validationmasks\n","!mkdir validationmasks\n","\n","!rm -rf testimages\n","!mkdir testimages\n","!rm -rf testmasks\n","!mkdir testmasks\n","\n","# Next we clone the git repository the data is stored in\n","!git clone https://github.com/slowvak/MC4-TensorflowUNet.git\n","\n","# Finally we unzip the files we've downloaded into the appropriate folders for\n","# training, validation, and testing images and segmentation masks\n","!unzip -q -o \"./MC4-TensorflowUNet/Pt1.zip\"\n","!unzip -q -o \"./MC4-TensorflowUNet/Pt2.zip\"\n","!mv *-Mask.jpg ./trainmasks\n","!mv *.jpg ./trainimages\n","\n","!unzip -q -o \"./MC4-TensorflowUNet/Pt3.zip\"\n","!mv *-Mask.jpg ./validationmasks\n","!mv *.jpg ./validationimages\n","\n","!unzip -q -o \"./MC4-TensorflowUNet/Pt4.zip\"\n","!mv *-Mask.jpg ./testmasks\n","!mv *.jpg ./testimages"],"id":"rFrVQ11_8Mcr","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4MgL2580DHIB"},"source":["## Creating a Custom `Dataset`"],"id":"4MgL2580DHIB"},{"cell_type":"markdown","metadata":{"id":"cyGMDw3XDSJg"},"source":["Creating custom `Dataset`s in PyTorch can be more or less straightforward depending on what format your files are saved in.  The data in this problem consists of .jpg files, so we will use the Python Imaging Library (PIL) to read the files.  Other formats, such as NIfTI or DICOM files, may require more complex behavior, such as recording statistics for volume normalization in the `__init__` method or assembling input data from only the appropriate parts of the file in the `__getitem__` method.\n","\n","A couple things that you should definitely avoid doing in `Dataset`s, as mentioned earlier, are pushing input data onto the GPU and most augmentation routines.  It's much more efficient to run augmentation on a GPU whenever possible, and much more efficient to push entire batches of data to the GPU instead of the single examples returned by a `Dataset`.  Instead, inside the training/validation/testing routines is where you should push the batches your `DataLoader` returns to the GPU, then run augmentation routines before feeding that batch into your model."],"id":"cyGMDw3XDSJg"},{"cell_type":"code","metadata":{"id":"1uOgRWL98oLp"},"source":["import torch\n","import torchvision\n","# we need a couple extra libraries to handle finding and opening our data files\n","import os\n","import PIL\n","\n","# defining a PyTorch Dataset is like defining any other new Python class\n","class SegmentationDataset(torch.utils.data.Dataset):\n","  # define the __init__ method that runs when the Dataset is created\n","  def __init__(self, image_dir, mask_dir):\n","    # image_dir represents the directory where the images are stored\n","    # mask_dir represents the directory where the masks are stored\n","\n","    # for this problem we'll store file paths for the images and masks in a list\n","    # sometimes dictionaries are better\n","    # particularly for data files containing multiple inputs each\n","    self.data_items = []\n","    \n","    # this defines the transforms we'll apply to the data\n","    # for normal image formats such as .jpg and .png torchvision provides routines\n","    # to handle normalizing and converting the data into Tensors\n","    # for other data formats these will most likely need to be programmed separately\n","    # in those cases, the code you write does not need to be wrapped in a torchvision.Transform\n","    # as long as program your dataset so the __getitem__ method returns a Tensor\n","    self.transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n","\n","    # using os.walk we look at every file in the image_dir directory\n","    for dirpath, _, files in os.walk(image_dir):\n","      for file in files:\n","        # record the path to the image file we're currently looking at\n","        image_path = os.path.join(dirpath, file)\n","        # because our data is saved with masks and images labeled identically\n","        # except for a `-Mask' tag, we can easily get the mask's filename from\n","        # the image's filename\n","        mask_file = file[:-4] + '-Mask.jpg'\n","        # now we can record the path to the mask file that corresponds\n","        # with the image we're looking at\n","        mask_path = os.path.join(mask_dir, mask_file)\n","        # finally we add a smaller list containing both paths to our list of data items\n","        self.data_items.append([image_path, mask_path])\n","  \n","  # the __getitem__ method is what the DataLoader calls to get one example of\n","  # input data and label/mask from the Dataset\n","  def __getitem__(self, index):\n","    # the DataLoader will give the Dataset an index\n","    # and the Dataset will return the corresponding data item\n","\n","    # image paths were stored at the 0th position in our image/mask pairs\n","    image_path = self.data_items[index][0]\n","    # and mask paths at the 1st position\n","    mask_path = self.data_items[index][1]\n","\n","    # PIL provides a method open the .jpg files our data is stored in\n","    # if your data was NIfTI images, for example, you would use methods\n","    # from NiBabel instead\n","    image_data = PIL.Image.open(image_path)\n","    mask_data = PIL.Image.open(mask_path)\n","\n","    # the torchvision transforms we called will convert PIL images into torch Tensors\n","    # because .jpg images are stored in UINT8 formats, their maximum values are 255\n","    # so we can standardize the X data to fall between 0 and 1 simply by dividing by 255\n","    image_tensor = self.transforms(image_data)/255.\n","    mask_tensor = self.transforms(image_data)\n","\n","    # finally, return the tensors containing our input data and label\n","    return image_tensor, mask_tensor\n","  \n","  # __len__ returns the total number of items in the Dataset\n","  # and is used by the DataLoader to determine how many batches it needs to assemble\n","  def __len__(self):\n","    # since we stored the data in a list we can simply use the len() function\n","    # organizing your data in a dictionary, for example, may require\n","    # a more complicated routine or special consideration during assembly\n","    return len(self.data_items)"],"id":"1uOgRWL98oLp","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f8hCll0e4L-R"},"source":["## Data Augmentation"],"id":"f8hCll0e4L-R"},{"cell_type":"markdown","metadata":{"id":"S9cQbr1H4Rsv"},"source":["Torchvision contains many [transforms](https://pytorch.org/vision/stable/transforms.html) that can be used for data augmentation.  While many of these can be used on PIL Images directly and composed with the `ToTensor()` transform to augment data in one step as it's being loaded, we'll instead write a new function that applies the augmentation to the data after it has been converted into a `Tensor`.\n","\n","There are a few advantages to doing it this way.  First, for medical imaging formats it allows you to skip the data type conversions needed to convert your data into .jpg or .png compatible images.  Second it gives you substantially more flexibility in your network architectures, for example you might want your model's input to be multiple adjacent slices from a CT scan or the entire 3-D scan.  Also worth mentioning, specifically for segmentation problems, we need to apply any spatial distortions we apply to the input data to the mask as well.  In this case we need to make sure that the mask still lines up with an image of the pancreas that our augmentation routine has rotated.  Creating an augmentation function like this makes that more straightforward."],"id":"S9cQbr1H4Rsv"},{"cell_type":"code","metadata":{"id":"IsRyQpb44Lc6"},"source":["# import random so we can randomly determine augmentation parameters\n","import random\n","\n","# for the input to this function we need both the input data and the labels\n","# other types of problem, such as classification, may only need the augmentation\n","# applied to the input data\n","def augment_fn(x_tensor, y_tensor):\n","  \"\"\"Randomly applies augmentations to the data.\"\"\"\n","  \n","  # first we'll do a bit of error handling\n","  # because we're going to iterate over the batch we want to double check\n","  # that the number of items in the batch matches\n","  # for both the input data and the masks\n","  if x_tensor.size()[0] != y_tensor.size()[0]:\n","    raise AttributeError(\"x and y tensors have different batch sizes\")\n","\n","  # to make sure it stays possible for our model to see the original, unaugmented data\n","  # and increase the different types of patterns the model will need to learn\n","  # we assign probabilities to each of the types of augmentation we intend to apply\n","  aug_prob = 0.9 # any sort of augmentation\n","  brightness_prob = 0.5 # brightness adjustments\n","  rot_prob = 0.8 # image rotation\n","  # setting the maximum allowable angle of rotation\n","  max_rot = 10. # degrees\n","\n","  # though it will be slower, we'll augment each item in the batch separately\n","  # to increase the diversity of training examples the model sees\n","  for batch_index in range(x_tensor.size()[0]):\n","    # first check if we're going to augment this example\n","    if random.random() < aug_prob:\n","      # if so, randomly choose the parameters that determine the degree of augmentation\n","      rot_angle = random.uniform(0., max_rot) # randomly pick the angle of rotation\n","      delta_brightness = random.uniform(0.95, 1.05) # randomly pick how much the brightness will be adjusted\n","      \n","      # this parameter is initialized to False so we only apply operations\n","      # to the mask if they happen to the image\n","      # in this case only rotation potentially needs to be applied to the mask\n","      rotate_mask = False\n","\n","      # check if we're going to augment the image brightness\n","      if random.random() < brightness_prob:\n","        # the functional version of the torch transform gives us the ability to\n","        # set random parameters such as the degree we'll augment the brightness\n","        x_tensor[batch_index,...] = torchvision.transforms.functional.adjust_brightness(img=x_tensor[batch_index,...], brightness_factor=delta_brightness)\n","      # check if we're going to rotate the image\n","      if random.random() < rot_prob:\n","        # similarly to above, the functional version of this torch transform\n","        # lets us pick how many degrees we'll rotate the image by\n","        # which is important because we need to rotate the mask by the same amount\n","        x_tensor[batch_index,...] = torchvision.transforms.functional.rotate(img=x_tensor[batch_index,...], angle=rot_angle, interpolation=torchvision.transforms.InterpolationMode.BILINEAR)\n","        # since we rotated the image we set this to True\n","        rotate_mask = True\n","\n","      # check if we need to rotate the mask\n","      if rotate_mask:\n","        # note that for mask rotation we use NEAREST interpolation instead of BILINEAR\n","        # because we want integer only values for our rotated segmentation masks\n","        y_tensor[batch_index,...] = torchvision.transforms.functional.rotate(img=y_tensor[batch_index,...], angle=rot_angle, interpolation=torchvision.transforms.InterpolationMode.NEAREST)\n","\n","  # finally, return the augmented tensors\n","  return x_tensor, y_tensor"],"id":"IsRyQpb44Lc6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DxOwpSWsD7Lx"},"source":["## Defining a U-Net Architecture"],"id":"DxOwpSWsD7Lx"},{"cell_type":"markdown","metadata":{"id":"RZlkUciFKwBc"},"source":["[U-Nets](https://en.wikipedia.org/wiki/U-Net) are the architecture typically used to solve segmentation problems.  Though this architecture is substantially more complex than the CIFAR-10 classification model we created earlier, the process used to create a U-Net from scratch in PyTorch is almost identical.\n","\n","The primary differences come from how U-Nets are typically organized.  While the previous model was built using only a few layers, U-Nets contain many layers organized into several blocks that focus their operations at a given spatial scale within the image.  Fortunately PyTorch allows programmers to create layers, specifically instances of the `torch.nn.Module` class, from collections of layers/`Module`s.\n","\n","Thanks to this feature, we can define the blocks we'll use to construct the U-Net as their own `Module`s, and make the code required to create the U-Net much simpler and more readable."],"id":"RZlkUciFKwBc"},{"cell_type":"code","metadata":{"id":"smoSMAfL8srp"},"source":["# defining the blocks that compose the first half of the U-Net\n","# which handle downsampling the data and creating the input for the skip-layers\n","class UNet_down_block(torch.nn.Module):\n","  # initialize the layers used for the down block\n","  def __init__(self, input_channel, output_channel, down_size):\n","    # super is needed when subclassing to make sure the torch.nn.Module\n","    # properties are applied to these new, custom classes\n","    super(UNet_down_block, self).__init__()\n","    # to keep our network small we'll make the downblocks from relatively few layers\n","    # with two convolutional layers followed by batch normalization layers\n","    # for these layers we'll use 3x3 convolutional kernels with 1x1 padding around the edges\n","    self.conv1 = torch.nn.Conv2d(input_channel, output_channel, 3, padding=1)\n","    self.bn1 = torch.nn.BatchNorm2d(output_channel)\n","    self.conv2 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)\n","    self.bn2 = torch.nn.BatchNorm2d(output_channel)\n","    self.max_pool = torch.nn.MaxPool2d(2, 2)\n","    self.relu = torch.nn.ReLU()\n","    # finally, a variable that tracks whether or not this downblock\n","    # should downsample its input before it analyzes the data\n","    self.down_size = down_size\n","\n","  # define what happens in this block during the forward pass for the U-Net\n","  def forward(self, x):\n","    # first pass the input through a MaxPool layer\n","    # if we're going to downsample in this block\n","    if self.down_size:\n","      x = self.max_pool(x)\n","    # next run the data through the two convolutional layers\n","    x = self.relu(self.bn1(self.conv1(x)))\n","    x = self.relu(self.bn2(self.conv2(x)))\n","    # finally return the data\n","    return x\n","\n","# the up blocks compose the second half of the U-Net\n","# and handle upsampling the data and interpreting the input from the skip layers\n","class UNet_up_block(torch.nn.Module):\n","  # initialize the layers used for the up block\n","  def __init__(self, prev_channel, input_channel, output_channel):\n","    super(UNet_up_block, self).__init__()\n","    # since we want to keep the network small we'll again only use two\n","    # convolutional layers with batch normalization per up block\n","    # again using 3x3 convolutional kernels and 1x1 padding around the edges\n","    # unlike last time we want to upsample within every up block so\n","    # we don't include a parameter to turn it off\n","    self.up_sampling = torch.nn.Upsample(scale_factor=2, mode='bilinear')\n","    self.conv1 = torch.nn.Conv2d(prev_channel + input_channel, output_channel, 3, padding=1)\n","    self.bn1 = torch.nn.BatchNorm2d(output_channel)\n","    self.conv2 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)\n","    self.bn2 = torch.nn.BatchNorm2d(output_channel)\n","    self.relu = torch.nn.ReLU()\n","\n","  # define what happens in this block during the forward pass for the U-Net\n","  def forward(self, prev_feature_map, x):\n","    # we up sample the input data first\n","    x = self.up_sampling(x)\n","    # then we concatenate the input from the previous layer\n","    # with the input from the skip connection\n","    x = torch.cat((x, prev_feature_map), dim=1)\n","    # then we run our data through the convolutional layers\n","    x = self.relu(self.bn1(self.conv1(x)))\n","    x = self.relu(self.bn2(self.conv2(x)))\n","    # and finally return it\n","    return x\n","\n","# with the down and up blocks defined we can define the entire U-Net architecture\n","class UNet(torch.nn.Module):\n","  # initialize the layers used to define the architecture\n","  def __init__(self):\n","    # like the blocks, the UNet needs to subclass the Module class\n","    # even though the classes it's constructed from already do\n","    super(UNet, self).__init__()\n","\n","    # first we define the 3 down blocks we'll use\n","    # to analyze our data at different spatial scales\n","    # we want the first down block to look at the data directly\n","    # so we don't tell it to down sample the data\n","    self.down_block1 = UNet_down_block(1, 16, False)\n","    # note that, as the spatial scale of our data shrinks, we\n","    # increase the number of features our network can use\n","    self.down_block2 = UNet_down_block(16, 32, True)\n","    self.down_block3 = UNet_down_block(32, 64, True)\n","\n","    # U-Nets also contain one or more convolutional layers\n","    # at the center that do some additional processing before\n","    # the data is upsampled again, in this case we'll only use one\n","    # this could also be defined as a center block like we did for the\n","    # down and up blocks, but since we're doing a relatively\n","    # simple operation in this network's center there's no need\n","    self.mid_conv1 = torch.nn.Conv2d(64, 64, 3, padding=1)\n","    self.bn1 = torch.nn.BatchNorm2d(64)\n","\n","    # finally we define the up blocks\n","    # note we need one less than the number of down blocks\n","    # because these are the counterparts to the down blocks\n","    # that down sampled the data as it passed through them\n","    self.up_block1 = UNet_up_block(32, 64, 32)\n","    self.up_block2 = UNet_up_block(16, 32, 16)\n","\n","    # and now the final \"block\" which will do the model's final interpretation\n","    # at the scale of the original image\n","    self.last_conv1 = torch.nn.Conv2d(16, 16, 3, padding=1)\n","    self.last_bn = torch.nn.BatchNorm2d(16)\n","    self.last_conv2 = torch.nn.Conv2d(16, 1, 1, padding=0)\n","\n","    # the activation function only needs to be defined once\n","    # so we'll just define it at the end\n","    self.relu = torch.nn.ReLU()\n","\n","  # define what happens during the forward pass for this U-Net\n","  def forward(self, x):\n","    # to create the pathways for the skip layers we create several variables\n","    # that store the output of the down blocks\n","    \n","    # this layer operates at the scale of the input data, scale: (1,1)\n","    self.x1 = self.down_block1(x)\n","    # downsampled once, scale (0.5, 0.5)\n","    self.x2 = self.down_block2(self.x1)\n","    # downsampled twice, scale (0.25,  0.25)\n","    self.x3 = self.down_block3(self.x2)\n","    self.x3 = self.relu(self.bn1(self.mid_conv1(self.x3)))\n","    # this will upsample the tensor back to scale (0.5, 0.5)\n","    x = self.up_block1(self.x2, self.x3)\n","    # and finally the tensor is upsampled back to scale (1, 1)\n","    x = self.up_block2(self.x1, x)\n","    x = self.relu(self.last_bn(self.last_conv1(x)))\n","    # note that the last convolutional layer does not undergo batch normalization\n","    # or a final activation, as before this is to preserve flexibility with\n","    # some of PyTorch's provided loss functions and calculations\n","    x = self.last_conv2(x)\n","    return x"],"id":"smoSMAfL8srp","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LWLwuMraLvF8"},"source":["Note that larger U-Nets are likely to have better results, and experiments with them can be done by simply inserting more convolutional layers, or any other kind of layer you'd like to experiment with.  Just make sure to keep track of what spatial scale each layer is operating on to avoid crashes during training."],"id":"LWLwuMraLvF8"},{"cell_type":"markdown","metadata":{"id":"NcXHqeQ_D_DT"},"source":["## Defining a Loss Function"],"id":"NcXHqeQ_D_DT"},{"cell_type":"markdown","metadata":{"id":"_1MLC-X9JSe6"},"source":["Custom loss functions in PyTorch can be defined simply by creating a function that calculates a loss value from `Tensor` inputs.  PyTorch loss functions typically have two versions, a functional version that defines the underlying mathematics, and a `torch.nn.modules.loss._Loss` version that typically calls the functional version.  While the latter isn't necessary, to emulate PyTorch native loss functions and make sure we have access to any related methods we might need from native PyTorch, we will use a loss function that subclasses `loss._Loss`.\n","\n","Dice loss, or 1 - the [Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient), is typically used for segmentation problems.  To keep the focus on PyTorch we'll use a standard \"soft\" Dice loss, which just compares the intersections of two areas to their union.  Since we're only segmenting a single class, we'll design a version for networks with sigmoid final activations.  We'll also design it to only look at the foreground, or class(es) we're predicting, so the model doesn't mistake the large background area as being more important than the pancreas."],"id":"_1MLC-X9JSe6"},{"cell_type":"code","metadata":{"id":"1KhPYY1A8yHy"},"source":["# this decorator tells PyTorch to attempt to compile the operations\n","# in the decorated function into one CUDA operation\n","# which can speed up multi-step calculations\n","# the function would run just as well without it, but may be slower\n","@torch.jit.script\n","# defining a function that calculates the soft Dice loss\n","def func_dice_loss_sigmoid(pred, targ):\n","  # pred consists of values in the range [0, 1]\n","  # this is what makes this Dice loss \"soft\"\n","  # the continuous loss calculations this results in give the model\n","  # the ability to smoothly improve its predictive ability\n","\n","  # \"hard\" Dice losses will apply a threshold to the model's predictions that\n","  # forces them to be either 0 or 1\n","  # this is useful as a metric on your validation and test sets\n","  # but makes the training process much less reliable\n","  # because the model's predictions become discontinuous\n","\n","  # targ will be either 0 (for background) or 1 (for the class we're predicting)\n","  # this should generally be true unless you are experimenting with label smoothing\n","  # which is outside the scope of this chapter\n","\n","  # this factor prevents divide-by-zero errors\n","  eps = 1.e-5\n","  # first transfer the inputs to the GPU\n","  # as the large matrix multiplications will otherwise be slow\n","  pred = pred.cuda()\n","  targ = targ.cuda()\n","  # calculate the interesection between the two Tensors\n","  inter = torch.sum(pred * targ)\n","  # calculate their union\n","  union = torch.sum(pred + targ)\n","  # calculate the Dice coefficient and subtract it from 1\n","  # to get the soft Dice loss\n","  loss = 1. - (2. * inter + eps) / (union + eps)\n","  # finally, return the loss\n","  return loss\n","\n","# subclassing PyTorch's native loss class so we can easily ensure\n","# our code plays nice with the underlying PyTorch calculations (e.g. backward())\n","class DiceLossSigmoid(torch.nn.modules.loss._Loss):\n","  # initialize this class as a loss._Loss subclass\n","  def __init__(self) -> None:\n","    super(DiceLossSigmoid, self).__init__()\n","\n","  # define the forward() method\n","  # which implicitly defines the backward() method\n","  # thanks to us subclassing the loss._Loss class\n","  def forward(self, preds, target):\n","    return func_dice_loss_sigmoid(preds, target)"],"id":"1KhPYY1A8yHy","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lv_tFKJkIlfC"},"source":["For multi-class segmentation problems, designed for the output from SoftMax activations, the calculations look similar, though the background becomes a represented class (typically class 0) and must be specifically excluded in your code if you encounter issues with your segmentation model predicting everything as background."],"id":"Lv_tFKJkIlfC"},{"cell_type":"markdown","metadata":{"id":"EHrm6zUHEDrQ"},"source":["## Setting Up the Training Process"],"id":"EHrm6zUHEDrQ"},{"cell_type":"markdown","metadata":{"id":"X3JJHRcHJZ6C"},"source":["For this example we'll add several components to the training process that typically improve the results and speed.  This gives us several more hyperparameters to set, so rather than hard-coding them as we did in the simpler example above, we'll define them all in one place so it's easier to keep track of their values and make changes when experimenting with different hyperparameter combinations."],"id":"X3JJHRcHJZ6C"},{"cell_type":"code","metadata":{"id":"Rbs8HDLZ8zgo"},"source":["# how many epochs we will train the model for\n","train_epochs = 5\n","# what the Adam optimizer's learning rate will be set to\n","# note that using a learning rate scheduler, as we are, overrides this value\n","adam_lr = 0.0003\n","# as we want to use a one-cycle training schedule we must set the scheduler's hyperparameters\n","# while it allows us to configure several hyperparameters\n","# we'll only hardcode the max learning rate and allow it to use\n","# default values for the other configurable parameters\n","max_lr = 0.03\n","# for this particular example, with relatively little preprocessing done on the dataset,\n","# multiprocessing is unlikely to noticeably speed up our data loading process\n","# but we'll include it for future reference\n","workers = 2\n","# finally, setting the batch size our DataLoaders will use\n","batchsize = 32"],"id":"Rbs8HDLZ8zgo","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m_Et8hnBJaQh"},"source":["As hinted in the code above, the implementation of a learning rate scheduler is one of the primary differences from the training process used in this example.  In this case, we'll use a [One-Cycle learning rate policy](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.OneCycleLR).  The second major difference is the use of [automated mixed precision](https://pytorch.org/docs/stable/amp.html).\n","\n","In short, what a One-Cycle policy does is start training the model with a small learning rate, increase to a very high learning rate over the first 1/3 of the training epochs, and finally slowly decrease it to a very small learning rate.  Using a high learning rate in this way helps regularize our model's parameters, making them more likely to avoid overfitting, and makes them converge faster.  However, it can make the model's training progress difficult to interpret as the loss frequently gets worse during the first half of the training cycle.\n","\n","\n","[Mixed precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html), on the other hand, doesn't modify hyperparameters but converts the underlying datatypes used for `Tensor` calculations between full (`float32`) and half (`float16`) precision at certain points in the training process.  The main advantage this brings is faster training, with some potential to help avoid overfitting.  One thing to note is that you can train using larger batch sizes (often by a factor of 2) using mixed precision and, if you decide to use it, you should test how much larger you can make your batches before running into GPU memory issues.  Automated mixed precision, as the name implies, will automatically manage the datatype conversions required in mixed precision training with just a few additions to the code."],"id":"m_Et8hnBJaQh"},{"cell_type":"code","metadata":{"id":"ZugY5dK081WI"},"source":["# create our U-Net on the GPU\n","unetmodel = UNet().cuda()\n","\n","# creating the training Dataset and DataLoader\n","# note that we've now defined the num_workers parameter, which turns on multiprocessing\n","trainset = SegmentationDataset('./trainimages', './trainmasks')\n","train_loader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, shuffle=True, num_workers=workers)\n","\n","# creating the validation Dataset and DataLoader\n","valset = SegmentationDataset('./validationimages', './validationmasks')\n","val_loader = torch.utils.data.DataLoader(valset, batch_size=batchsize, shuffle=True, num_workers=workers)\n","\n","# creating the test Dataset and DataLoader\n","testset = SegmentationDataset('./testimages', './testmasks')\n","test_loader = torch.utils.data.DataLoader(testset, batch_size=batchsize, shuffle=True, num_workers=workers)\n","\n","# setting our custom Dice loss function as the optimization criterion for our model\n","criterion = DiceLossSigmoid()\n","\n","# setting Adam as the gradient optimizer for the training process\n","optimizer = torch.optim.Adam(unetmodel.parameters(), lr=adam_lr)\n","# setting OneCycleLR as our learning rate scheduler\n","# note that this requires as input the optimizer it will be operating on\n","# the maximum learning rate it is allowed to set which determines the initial and final learning rates\n","# alongside the div_factor and final_div_factor parameters we've left at the default\n","# and some information that tells it when it should adjust the learning rate\n","# in this case we tell it how many epochs we will train for and how many steps it should take per epoch\n","# which in this case is one step per batch\n","# though there's also the option to tell it the total number of steps it should take instead\n","scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, epochs=train_epochs, steps_per_epoch=len(train_loader))\n","\n","# finally creating the scaler that we need for automated mixed precision training\n","scaler = torch.cuda.amp.GradScaler()"],"id":"ZugY5dK081WI","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZMUaGbcENqZ"},"source":["## Building a Training Loop with Automated Mixed Precision"],"id":"uZMUaGbcENqZ"},{"cell_type":"markdown","metadata":{"id":"w4uIHAqzJYiJ"},"source":["Now that we've set the hyperparameters and created all of the components needed for the training process, it's time to define the training and validation functions.  There are a couple new parameters added to the training function this time: a `lr_scheduler` parameter, so we can pass in the learning rate scheduler we defined earlier, and a `grad_scaler` parameter, so we can pass in the gradient scaler that helps manage automated mixed precision for us.  For flexibility, both parameters have been made optional and the training loop will run as is with either parameter missing.\n","\n","[As learning rate schedulers adjust the optimizer, there is a question when they should adjust, or step, the optimizer](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate).  For `OneCycleLR`, that's after every batch, so we include it in our training function to step the optimizer at the end of each batch.  Other schedulers are designed to step the optimizer at the end of each epoch (or potentially after several epochs) and you should instead step them after the validation loop.  This is shown in a line of commented out code when we run the actual training process.\n","\n","Gradient scalers are implemented in a [pretty particular way](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html).  A short description is that you must tell PyTorch to `autocast()` the output of your model and loss function, then use the scaler to wrap the `backward()` pass, the optimizer `step()`, and your model's weight `update()` to help prevent divergent behavior.  It's a simple process that's difficult to describe well.  It's much easier to understand by looking at examples, see the code below."],"id":"w4uIHAqzJYiJ"},{"cell_type":"code","metadata":{"id":"LndqsOz488tp"},"source":["# Defining what happens during one epoch's worth of training\n","# Passing in our DataLoader, model, loss function, etc.\n","# like this is not a problem for PyTorch\n","def train_pass(data_loader, model, loss_fn, optim, lr_scheduler = None, grad_scaler = None):\n","  # first we make sure the model is in train mode\n","  # so it will track gradients and unfreeze parameters\n","  model.train()\n","  # we define a running loss variable so we can track our model's progress\n","  running_loss =  0.0\n","  # now we define what happens as the model goes through the data within the DataLoader\n","  # this will iterate over the batches defined within the DataLoader\n","  for i, data in enumerate(data_loader, 0):\n","    # this is a more efficient way to reset the gradient\n","    # though there may be cases where other parts of your code are incompatible\n","    # see: https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.zero_grad\n","    for param in model.parameters():\n","      param.grad = None\n","\n","    # using automated mixed precision\n","    if grad_scaler is not None:\n","      # read the inputs, a list of [inputs, masks], from the batch\n","      inputs, masks = data\n","      # because we want to train on the GPU\n","      # we must send our inputs and masks to the GPU\n","      # comment out both of these lines if you prefer to run on the CPU\n","      inputs = inputs.cuda()\n","      # for mixed precision to work we need to ensure the data type\n","      # of our masks is float 32 and convert them if not\n","      masks = masks.long().cuda()\n","\n","      # augment the data\n","      # augmentation should be run in such a way that it doesn't affect the model's gradients\n","      # so this function is enclosed within torch.no_grad()\n","      with torch.no_grad():\n","        inputs, masks = augment_fn(inputs, masks)\n","\n","      # before we feed the input to the model we tell PyTorch to autocast\n","      # the enclosed operations into the appropriate datatypes where applicable\n","      # where the operations may be a convolutional layer, other tensor operation\n","      # or something else\n","      with torch.cuda.amp.autocast():\n","        # get the outputs by sending our inputs through one forward pass of the model\n","        # note the outputs of the model are first run through a sigmoid function\n","        # as the loss function we created does not include a final activation\n","        outputs = torch.sigmoid(model(inputs))\n","\n","        # calculate the loss by comparing the model's outputs to the corresponding masks\n","        loss = loss_fn(outputs, masks)\n","\n","      # first, scale the loss so that the gradients generated by backward() are scaled\n","      grad_scaler.scale(loss).backward()\n","      # scaler.step() unscales the gradients and calls the optimizer\n","      # if the unscaled gradients are all finite numbers\n","      # otherwise it skips stepping the optimizer\n","      grad_scaler.step(optim)\n","      # finally, update the scaler's scale for the next batch\n","      grad_scaler.update()\n","\n","    # the training loop when there is no gradient scaler\n","    # effectively unchanged from the previous example\n","    # except where necessary for the different loss function etc. needed\n","    else:\n","      # read the inputs, a list of [inputs, masks], from the batch\n","      inputs, masks = data\n","      # because we want to train on the GPU\n","      # we must send our inputs and masks to the GPU\n","      # comment out both of these lines if you prefer to run on the CPU\n","      inputs = inputs.cuda()\n","      masks = masks.cuda()\n","\n","      # augment the data\n","      inputs, masks = augment_fn(inputs, masks)\n","\n","      # get the outputs by sending our inputs through one forward pass of the model\n","      # note the outputs of the model are first run through a sigmoid function\n","      # as the loss function we created does not include a final activation\n","      outputs = torch.sigmoid(model(inputs))\n","\n","      # calculate the loss by comparing the model's outputs to the corresponding masks\n","      loss = loss_fn(outputs, masks)\n","      # backward() calculates the gradient from the model's loss and stores it in\n","      # the model's tensors' gradient attribute\n","      loss.backward()\n","      # step() tells the optimizer to update our model's parameters\n","      optim.step()\n","\n","    # add the loss for the current batch to our running loss variable\n","    running_loss += loss.item()\n","\n","    # explicitly clearing memory to prevent memory leaks\n","    del inputs, masks, outputs\n","\n","    # this controls how we want the training loop to report our model's progress\n","    # in this case every 10 batches we want it to print, then reset, the average loss\n","    if i % 10 == 9:\n","      print('Training loss: ' + str(running_loss/10))\n","      running_loss = 0.0\n","    \n","    # stepping the learning rate scheduler, for those that are stepped after each batch\n","    if lr_scheduler is not None:\n","      scheduler.step()\n","\n","  # because all of the parameters that are updated are defined\n","  # outside the training loop we don't need this function to return anything\n","  # this line doesn't actually do anything, but is included just to make it clear\n","  # where this function ends\n","  return None"],"id":"LndqsOz488tp","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GUa3de_5JX15"},"source":["While the training function requires a few changes to account for the additions to our training process, the validation function from before will work almost exactly as is.  We don't intend to update the model during this process so there is nothing for the learning rate scheduler or the gradient scaler to do in this function and no need to invoke `autocast()`.  All we need to do is change the loss function and make sure the model's outputs go through a sigmoid final activation."],"id":"GUa3de_5JX15"},{"cell_type":"code","metadata":{"id":"PfQ11oN58_Bh"},"source":["# Now we'll define what happens when an epoch reaches the validation stage\n","# Note that we don't need to pass in the optimizer because we don't intend\n","# to update our model's parameters\n","def valid_pass(data_loader, model, loss_fn):\n","  # first we set the model to eval mode\n","  # this disables several under the hood operations that we won't need like gradient tracking\n","  # and will speed up the calculations within\n","  model.eval()\n","  # like before we define a loss variable to keep track of how our model performs\n","  val_loss = 0.0\n","\n","  # because we don't want to calculate or track any gradients during validation\n","  # we tell PyTorch not to track any gradients for these calculations using\n","  # with torch.no_grad():\n","  with torch.no_grad():\n","    # now we define what happens as our model runs through the validation DataLoader\n","    for i, data in enumerate(data_loader, 0):\n","      # read the inputs, a list of [inputs, masks], from the batch\n","      inputs, masks = data\n","      # because we want to train on the GPU\n","      # we must send our inputs and masks to the GPU\n","      # comment out both of these lines if you prefer to run on the CPU\n","      inputs = inputs.cuda()\n","      masks = masks.cuda()\n","\n","      # Note the lack of a backwards pass in the validation loop\n","      # because we're not going to perform any parameter updates or gradient calculations\n","\n","      # get the outputs by sending our inputs through one forward pass of the model\n","      outputs = torch.sigmoid(model(inputs))\n","      # calculate the loss by comparing our model's outputs to their masks\n","      # note: if our loss function did not include a softmax operation (or other final activation)\n","      # we would need to make sure to calculate it before calculating the loss\n","      loss = loss_fn(outputs, masks)\n","\n","      # add the loss for the current batch to our validation loss\n","      val_loss += loss.item()\n","\n","      # explicitly clearing memory to prevent memory leaks\n","      del inputs, masks, outputs\n","\n","  # Finally we'll have our validation loop print the average loss of a validation batch\n","  print('Validation loss: ' + str(val_loss / len(data_loader)))\n","\n","  # this again does nothing but mark where the function ends\n","  return None"],"id":"PfQ11oN58_Bh","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5AkAfybHEJUJ"},"source":["## Training and Testing our Model"],"id":"5AkAfybHEJUJ"},{"cell_type":"markdown","metadata":{"id":"6CmpRwTxJVgB"},"source":["As before actually training our model is relatively simple after defining our training and validation functions.  The only changes are that hardcoded parameters, such as the number of epochs have been replaced by the variables defined above and the `train_pass` function has a few extra parameters passed into it.  There's also a line indicating where to step a learning rate scheduler that is stepped every epoch instead of every batch."],"id":"6CmpRwTxJVgB"},{"cell_type":"code","metadata":{"id":"YInQAyzM9ChC"},"source":["# Now we will actually run through model training\n","for epoch in range(train_epochs):\n","  print('Epoch Number: ' + str(epoch))\n","  print('Entering training loop:')\n","  train_pass(train_loader, unetmodel, criterion, optimizer, scheduler, scaler)\n","  print('Entering validation loop:')\n","  valid_pass(val_loader, unetmodel, criterion)\n","  # OneCycleLR needs to be stepped after each batch\n","  # but other schedulers are stepped after each epoch\n","  # learning rate schedulers that step after epochs should call step() here\n","  #scheduler.step()\n","print('Finished Training')"],"id":"YInQAyzM9ChC","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KK7NtCYhJWLp"},"source":["The testing loop is again very similar to the validation loop, with the only changes being to the loss function and the model's final activation, as specified by the different task we're testing."],"id":"KK7NtCYhJWLp"},{"cell_type":"code","metadata":{"id":"mgboX1FN9FCi"},"source":["# as with the validation set we don't want PyTorch to calculate gradients\n","with torch.no_grad():\n","  running_loss = 0.0\n","  # iterating over the test  set DataLoader\n","  for data in test_loader:\n","    # grab the images and masks from the batch\n","    inputs, masks = data\n","    inputs = inputs.cuda()\n","    masks = masks.cuda()\n","    # record the model's outputs\n","    outputs = torch.sigmoid(unetmodel(inputs))\n","    running_loss += DiceLossSigmoid()(outputs, masks).item()\n","  \n","  avg_loss = running_loss / len(test_loader)\n","  print(\"Test Loss: \" + str(avg_loss))"],"id":"mgboX1FN9FCi","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EID2G2IcBYP5"},"source":[""],"id":"EID2G2IcBYP5","execution_count":null,"outputs":[]}]}
